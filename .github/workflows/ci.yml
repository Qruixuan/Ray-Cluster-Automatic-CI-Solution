name: Ray-Cluster-Automatic-CI

on:
  push:
    branches: [ "main" ]

jobs:
  build-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v5.0.0

      - name: Set up Kind
        uses: engineerd/setup-kind@v0.6.2

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4.3.0

      - name: Create kind cluster
        run: |
          kind create cluster --image=kindest/node:v1.26.0 --name test-cluster
          kubectl cluster-info
        
      - name: Add KubeRay Helm repo
        run: |
          helm repo add kuberay https://ray-project.github.io/kuberay-helm/
          helm repo update

      - name: Deploy kuberay
        run: |
          helm upgrade --install kuberay-operator kuberay/kuberay-operator --version 1.4.2 --namespace default --create-namespace
      
      - name: Wait for RayCluster CRD
        run: |
          for i in {1..20}; do
            kubectl get crd rayclusters.ray.io -o yaml | grep -A5 "version:" && break
            echo "Waiting for RayCluster CRD..."
            sleep 3
          done    

      - name: Add Prometheus Helm repo
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
      
      - name: Install/Upgrade kube-prometheus-stack
        run: |
          helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
            --namespace monitoring --create-namespace \
            --set grafana.enabled=false

      - name: Apply Ray Cluster
        run: kubectl apply -f ./k8s/ray-cluster.yaml

      - name: Apply PodMonitor
        run: kubectl apply -f ./k8s/podmonitor.yaml

      - name: Create ConfigMap for task
        run: |
          kubectl create configmap ray-test-script \
            --from-file=my_task.py=./my_task.py \
            -n default --dry-run=client -o yaml | kubectl apply -f -

      - name: Apply Test Job
        run: kubectl apply -f ./k8s/test-job.yaml

      - name: Scrape metrics while Job runs
        run: |
          JOB_NAME=ray-test-job
          POD=""
          for i in {1..20}; do
            POD=$(kubectl get pods -n default -l job-name=$JOB_NAME -o jsonpath='{.items[0].metadata.name}')
            if [ -n "$POD" ]; then break; fi
            sleep 1
          done
          echo "Found pod: $POD"

          for i in {1..30}; do
            STATUS=$(kubectl get pod $POD -n default -o jsonpath='{.status.phase}')
            if [ "$STATUS" == "Running" ]; then
              echo "Pod is running"
              break
            fi
            echo "Waiting for pod to be running..."
            sleep 2
          done

          kubectl port-forward -n default $POD 8000:8000 &
          PF_PID=$!
          sleep 2 

          for i in {1..20}; do
            curl -s http://127.0.0.1:8000/metrics >/dev/null 2>&1
            if [ $? -eq 0 ]; then
              echo "Metrics server is ready"
              break
            fi
            echo "Waiting for metrics server..."
            sleep 1
          done

          echo "Scraping metrics 10 times:"
          for j in {1..10}; do
            curl -s http://127.0.0.1:8000/metrics | grep ray_test_metric || true
            sleep 1
          done
          kill $PF_PID

      - name: Wait for Job completion
        run: |
          JOB_NAME=ray-test-job
          kubectl wait --for=condition=complete --timeout=120s job/$JOB_NAME || true

      - name: Get Job logs
        run: |
          POD=$(kubectl get pods -n default -l job-name=ray-test-job -o jsonpath='{.items[0].metadata.name}')
          kubectl logs $POD

      - name: Delete Kind cluster
        if: always()
        run: |
          kind delete cluster --name test-cluster
